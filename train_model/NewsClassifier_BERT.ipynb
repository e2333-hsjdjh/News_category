{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab224eb0",
   "metadata": {},
   "source": [
    "## 1. 准备环境与导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53466b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps (Apple Silicon)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "\n",
    "# 2. 设置随机种子\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 3. 选择设备\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using device: mps (Apple Silicon)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using device: cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device: cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9947fac0",
   "metadata": {},
   "source": [
    "## 2. 数据读取与处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8b7bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 209527\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "data_path = \"../DATA/dataset.json\"\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    records = json.load(f)\n",
    "print(f\"Total samples: {len(records)}\")\n",
    "\n",
    "# --- BERT Dataset ---\n",
    "class NewsBertDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def prepare_bert_data(records, text_key, tokenizer, max_len=64):\n",
    "    df = pd.DataFrame(records)\n",
    "    texts = df[text_key].fillna(\"\").tolist()\n",
    "    labels_raw = df[\"category\"].tolist()\n",
    "    \n",
    "    # Label Mapping\n",
    "    unique_labels = sorted(set(labels_raw))\n",
    "    label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "    labels = [label2id[l] for l in labels_raw]\n",
    "\n",
    "    dataset = NewsBertDataset(texts, labels, tokenizer, max_len)\n",
    "    \n",
    "    # Split\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(0.7 * total_size)\n",
    "    val_size = int(0.15 * total_size)\n",
    "    meta_size = total_size - train_size - val_size\n",
    "    \n",
    "    train_ds, val_ds, meta_ds = random_split(\n",
    "        dataset, [train_size, val_size, meta_size], \n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    return train_ds, val_ds, meta_ds, label2id, id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce434a85",
   "metadata": {},
   "source": [
    "## 3. BERT 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34022c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertNewsClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BertNewsClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        # 使用 pooled_output (CLS token 的 embedding 经过一个线性层和 Tanh)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f11045",
   "metadata": {},
   "source": [
    "## 4. 训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f89558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples, epoch=1, num_epochs=1, log_every=2000):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for step, d in enumerate(data_loader, start=1):\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if step % log_every == 0:\n",
    "             print(f\"Epoch {epoch}/{num_epochs} | Step {step}/{len(data_loader)} | loss={loss.item():.4f}\")\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "def run_bert_training(text_key, save_name, epochs=2, batch_size=16):\n",
    "    print(f\"\\n{'='*20} Training BERT on {text_key} {'='*20}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    train_ds, val_ds, meta_ds, label2id, id2label = prepare_bert_data(records, text_key, tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "    \n",
    "    model = BertNewsClassifier(len(label2id)).to(device)\n",
    "    \n",
    "    # 使用 torch.optim.AdamW 替代 transformers.AdamW\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    history = {\n",
    "        'train_acc': [],\n",
    "        'train_loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        train_acc, train_loss = train_epoch(\n",
    "            model, train_loader, loss_fn, optimizer, device, scheduler, len(train_ds), epoch=epoch+1, num_epochs=epochs\n",
    "        )\n",
    "        print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "        val_acc, val_loss = eval_model(\n",
    "            model, val_loader, loss_fn, device, len(val_ds)\n",
    "        )\n",
    "        print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "        \n",
    "        # Record history\n",
    "        history['train_acc'].append(train_acc.item())\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_acc'].append(val_acc.item())\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'label2id': label2id,\n",
    "                'id2label': id2label,\n",
    "                'config': {'text_key': text_key}\n",
    "            }, save_name)\n",
    "            print(f\"Saved best model to {save_name}\")\n",
    "            \n",
    "    end_time = time.time()\n",
    "    print(f\"Training finished in {end_time - start_time:.2f}s\")\n",
    "    return best_accuracy, history\n",
    "\n",
    "def plot_training_history(history, title=\"Training History\"):\n",
    "    acc = history['train_acc']\n",
    "    val_acc = history['val_acc']\n",
    "    loss = history['train_loss']\n",
    "    val_loss = history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo-', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'ro-', label='Validation acc')\n",
    "    plt.title(f'{title} - Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
    "    plt.title(f'{title} - Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a86e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Training BERT on headline ====================\n"
     ]
    }
   ],
   "source": [
    "# --- 运行训练 (串行，因为 BERT 显存占用大) ---\n",
    "# 确保目录存在\n",
    "os.makedirs(\"../Results/bert\", exist_ok=True)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "# 训练 Headline 模型\n",
    "acc_h, history_h = run_bert_training(\n",
    "    text_key=\"headline\", \n",
    "    save_name=\"../Results/bert/best_bert_headline.pt\", \n",
    "    epochs=epochs, \n",
    "    batch_size=16 # 如果显存不足，调小此值\n",
    ")\n",
    "plot_training_history(history_h, title=\"BERT Headline Model\")\n",
    "\n",
    "# 训练 Description 模型\n",
    "acc_d, history_d = run_bert_training(\n",
    "    text_key=\"short_description\", \n",
    "    save_name=\"../Results/bert/best_bert_description.pt\", \n",
    "    epochs=epochs, \n",
    "    batch_size=16\n",
    ")\n",
    "plot_training_history(history_d, title=\"BERT Description Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d56377",
   "metadata": {},
   "source": [
    "## 5. Meta Model (Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12793719",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), \n",
    "            nn.ReLU(),                        \n",
    "            nn.Dropout(0.2),                  \n",
    "            nn.Linear(hidden_dim, output_dim) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def load_bert_model(path, device):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    label2id = checkpoint['label2id']\n",
    "    model = BertNewsClassifier(len(label2id))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, label2id\n",
    "\n",
    "def generate_meta_features(records, indices, model_h, model_d, tokenizer, label2id, device):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    print(f\"Generating meta features for {len(indices)} samples...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in indices:\n",
    "            item = records[idx]\n",
    "            if item['category'] not in label2id: continue\n",
    "            target = label2id[item['category']]\n",
    "            \n",
    "            # Headline Pred\n",
    "            enc_h = tokenizer.encode_plus(\n",
    "                str(item.get('headline', '')), max_length=64, padding='max_length', truncation=True, return_tensors='pt'\n",
    "            )\n",
    "            out_h = model_h(enc_h['input_ids'].to(device), enc_h['attention_mask'].to(device))\n",
    "            probs_h = F.softmax(out_h, dim=1).cpu().tolist()[0]\n",
    "            \n",
    "            # Desc Pred\n",
    "            enc_d = tokenizer.encode_plus(\n",
    "                str(item.get('short_description', '')), max_length=64, padding='max_length', truncation=True, return_tensors='pt'\n",
    "            )\n",
    "            out_d = model_d(enc_d['input_ids'].to(device), enc_d['attention_mask'].to(device))\n",
    "            probs_d = F.softmax(out_d, dim=1).cpu().tolist()[0]\n",
    "            \n",
    "            X.append(probs_h + probs_d)\n",
    "            y.append(target)\n",
    "            \n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# --- Train Meta Model ---\n",
    "# 1. Load Models\n",
    "model_h, label2id_h = load_bert_model(\"../Results/bert/best_bert_headline.pt\", device)\n",
    "model_d, label2id_d = load_bert_model(\"../Results/bert/best_bert_description.pt\", device)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 2. Get Meta Data Indices\n",
    "full_ds_size = len(records)\n",
    "train_size = int(0.7 * full_ds_size)\n",
    "val_size = int(0.15 * full_ds_size)\n",
    "meta_size = full_ds_size - train_size - val_size\n",
    "_, _, meta_subset = random_split(\n",
    "    range(full_ds_size), [train_size, val_size, meta_size], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# 3. Generate Features\n",
    "X_meta, y_meta = generate_meta_features(records, meta_subset.indices, model_h, model_d, tokenizer, label2id_h, device)\n",
    "\n",
    "# 4. Train Loop\n",
    "meta_ds = torch.utils.data.TensorDataset(X_meta, y_meta)\n",
    "meta_loader = DataLoader(meta_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "meta_model = MetaClassifier(len(label2id_h)*2, 64, len(label2id_h)).to(device)\n",
    "optimizer = torch.optim.Adam(meta_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Training Meta Model...\")\n",
    "for epoch in range(10):\n",
    "    meta_model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in meta_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = meta_model(X_batch)\n",
    "        loss = criterion(out, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss {total_loss/len(meta_loader):.4f}\")\n",
    "\n",
    "torch.save(meta_model.state_dict(), \"../Results/bert/best_meta_model_bert.pt\")\n",
    "print(\"Meta Model Saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc09ab23",
   "metadata": {},
   "source": [
    "## 6. 推理与使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46505dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEnsemblePredictor:\n",
    "    def __init__(self, model_dir=\"Results/bert\", device=device):\n",
    "        self.device = device\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Load Headline Model\n",
    "        self.model_h, self.label2id = load_bert_model(f\"{model_dir}/best_bert_headline.pt\", device)\n",
    "        self.id2label = {v: k for k, v in self.label2id.items()}\n",
    "        \n",
    "        # Load Desc Model\n",
    "        self.model_d, _ = load_bert_model(f\"{model_dir}/best_bert_description.pt\", device)\n",
    "        \n",
    "        # Load Meta Model\n",
    "        self.meta_model = MetaClassifier(len(self.label2id)*2, 64, len(self.label2id)).to(device)\n",
    "        self.meta_model.load_state_dict(torch.load(f\"{model_dir}/best_meta_model_bert.pt\", map_location=device))\n",
    "        self.meta_model.eval()\n",
    "\n",
    "    def predict(self, headline, description):\n",
    "        # Headline Probs\n",
    "        enc_h = self.tokenizer.encode_plus(\n",
    "            headline, max_length=64, padding='max_length', truncation=True, return_tensors='pt'\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            out_h = self.model_h(enc_h['input_ids'].to(self.device), enc_h['attention_mask'].to(self.device))\n",
    "            probs_h = F.softmax(out_h, dim=1)\n",
    "\n",
    "        # Desc Probs\n",
    "        enc_d = self.tokenizer.encode_plus(\n",
    "            description, max_length=64, padding='max_length', truncation=True, return_tensors='pt'\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            out_d = self.model_d(enc_d['input_ids'].to(self.device), enc_d['attention_mask'].to(self.device))\n",
    "            probs_d = F.softmax(out_d, dim=1)\n",
    "\n",
    "        # Meta Ensemble\n",
    "        meta_input = torch.cat([probs_h, probs_d], dim=1)\n",
    "        with torch.no_grad():\n",
    "            final_logits = self.meta_model(meta_input)\n",
    "            final_probs = F.softmax(final_logits, dim=1)\n",
    "            \n",
    "        pred_idx = torch.argmax(final_probs, dim=1).item()\n",
    "        return self.id2label[pred_idx]\n",
    "\n",
    "# --- Test ---\n",
    "try:\n",
    "    predictor = BertEnsemblePredictor()\n",
    "    sample_h = \"NASA announces new mission to Mars\"\n",
    "    sample_d = \"The space agency plans to send humans to the red planet by 2030.\"\n",
    "    print(f\"Prediction: {predictor.predict(sample_h, sample_d)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please ensure models are trained first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
