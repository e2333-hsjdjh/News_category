{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7934610f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps (Apple Silicon)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. 设置随机种子\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 2. 选择设备\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using device: mps (Apple Silicon)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using device: cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device: cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd02590e",
   "metadata": {},
   "source": [
    "## 1. 数据处理 (Data Processing)\n",
    "沿用之前的分词器和数据集逻辑，但为了适应更大的模型，我们可以稍微增加词表大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de267058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 209527\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "data_path = \"../DATA/dataset.json\"\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    records = json.load(f)\n",
    "print(f\"Total samples: {len(records)}\")\n",
    "\n",
    "# --- Tokenizer ---\n",
    "class WhitespaceTokenizer:\n",
    "    def __init__(self, max_vocab_size: int = 50000, min_freq: int = 2, max_length: int = 128):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_freq = min_freq\n",
    "        self.max_length = max_length\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "        self.cls_token = \"<cls>\"\n",
    "        self.token_to_id = {self.pad_token: 0, self.unk_token: 1, self.cls_token: 2}\n",
    "        self.id_to_token = {0: self.pad_token, 1: self.unk_token, 2: self.cls_token}\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        text = str(text).lower().strip()\n",
    "        return re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "\n",
    "    def build_vocab(self, texts: List[str]) -> None:\n",
    "        counter = Counter()\n",
    "        for t in texts:\n",
    "            counter.update(self.tokenize(t))\n",
    "        most_common = [w for w, c in counter.most_common(self.max_vocab_size) if c >= self.min_freq]\n",
    "        for idx, token in enumerate(most_common, start=len(self.token_to_id)):\n",
    "            if token not in self.token_to_id:\n",
    "                self.token_to_id[token] = idx\n",
    "                self.id_to_token[idx] = token\n",
    "\n",
    "    def encode(self, text: str) -> Tuple[List[int], List[int]]:\n",
    "        tokens = [self.cls_token] + self.tokenize(text)\n",
    "        tokens = tokens[: self.max_length]\n",
    "        ids = [self.token_to_id.get(tok, self.token_to_id[self.unk_token]) for tok in tokens]\n",
    "        pad_length = self.max_length - len(ids)\n",
    "        if pad_length > 0:\n",
    "            ids += [self.token_to_id[self.pad_token]] * pad_length\n",
    "        attention_mask = [1 if i != self.token_to_id[self.pad_token] else 0 for i in ids]\n",
    "        return ids, attention_mask\n",
    "\n",
    "# --- Dataset ---\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        # Pre-tokenize all texts to save CPU time during training\n",
    "        # 预分词：在初始化时处理所有文本，避免在训练循环中重复计算\n",
    "        print(\"Pre-tokenizing dataset (this may take a moment)...\")\n",
    "        self.input_ids = []\n",
    "        self.attention_masks = []\n",
    "        \n",
    "        for t in texts:\n",
    "            ids, mask = tokenizer.encode(t)\n",
    "            self.input_ids.append(ids)\n",
    "            self.attention_masks.append(mask)\n",
    "            \n",
    "        self.input_ids = torch.tensor(self.input_ids, dtype=torch.long)\n",
    "        self.attention_masks = torch.tensor(self.attention_masks, dtype=torch.long)\n",
    "        print(\"Pre-tokenization complete.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 直接返回预处理好的 Tensor，速度更快\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_masks[idx],\n",
    "            \"labels\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "def prepare_data(records, text_key, tokenizer):\n",
    "    df = pd.DataFrame(records)\n",
    "    texts = df[text_key].fillna(\"\").tolist()\n",
    "    labels_raw = df[\"category\"].tolist()\n",
    "    \n",
    "    unique_labels = sorted(set(labels_raw))\n",
    "    label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "    labels = [label2id[l] for l in labels_raw]\n",
    "    \n",
    "    tokenizer.build_vocab(texts)\n",
    "    dataset = NewsDataset(texts, labels, tokenizer)\n",
    "    \n",
    "    total = len(dataset)\n",
    "    train_size = int(0.7 * total)\n",
    "    val_size = int(0.15 * total)\n",
    "    meta_size = total - train_size - val_size\n",
    "    \n",
    "    train_ds, val_ds, meta_ds = random_split(\n",
    "        dataset, [train_size, val_size, meta_size], \n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    return train_ds, val_ds, meta_ds, label2id, id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818662cb",
   "metadata": {},
   "source": [
    "## 2. 高级模型架构 (Advanced Architecture)\n",
    "\n",
    "### 核心组件：\n",
    "1.  **RMSNorm**: 比 LayerNorm 更简单且效果相当或更好。\n",
    "2.  **RoPE (Rotary Positional Embedding)**: 将绝对位置信息编码为旋转矩阵，使得 Attention 能够自然地捕捉相对位置信息。\n",
    "3.  **SwiGLU**: 改进的前馈神经网络激活函数。\n",
    "4.  **Custom Transformer Encoder**: 手动堆叠 Block 以集成上述组件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8852a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. RMSNorm ---\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq, dim)\n",
    "        var = torch.mean(x ** 2, dim=-1, keepdim=True)\n",
    "        x_norm = x * torch.rsqrt(var + self.eps)\n",
    "        return self.weight * x_norm\n",
    "\n",
    "# --- 2. RoPE (Rotary Positional Embedding) ---\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        # 计算 theta\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self._update_cos_sin_tables(max_seq_len)\n",
    "\n",
    "    def _update_cos_sin_tables(self, seq_len):\n",
    "        t = torch.arange(seq_len, device=self.inv_freq.device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: (batch, n_heads, seq_len, head_dim)\n",
    "        if seq_len > self.cos_cached.shape[2]:\n",
    "            self._update_cos_sin_tables(seq_len)\n",
    "        \n",
    "        return (\n",
    "            self.cos_cached[:, :, :seq_len, ...], \n",
    "            self.sin_cached[:, :, :seq_len, ...]\n",
    "        )\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    # q, k: (batch, n_heads, seq_len, head_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "# --- 3. Multi-Head Attention with RoPE ---\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, max_len=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = d_model // n_head\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.rope = RotaryEmbedding(self.head_dim, max_seq_len=max_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        q = self.q_proj(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Apply RoPE\n",
    "        cos, sin = self.rope(v, seq_len=T)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        \n",
    "        # Attention Score\n",
    "        # (B, H, T, D) @ (B, H, D, T) -> (B, H, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            # attention_mask: (B, T) -> (B, 1, 1, T)\n",
    "            # 0 means padding, we want to mask them out (set to -inf)\n",
    "            mask = attention_mask[:, None, None, :]\n",
    "            att = att.masked_fill(mask == 0, float('-inf'))\n",
    "            \n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        \n",
    "        # Weighted Sum\n",
    "        y = att @ v # (B, H, T, T) @ (B, H, T, D) -> (B, H, T, D)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        return self.out_proj(y)\n",
    "\n",
    "# --- 4. Feed Forward (SwiGLU variant) ---\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.w2 = nn.Linear(hidden_dim, d_model)\n",
    "        self.w3 = nn.Linear(d_model, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # SwiGLU: (Swish(xW1) * xW3) W2\n",
    "        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))\n",
    "\n",
    "# --- 5. Transformer Block ---\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(d_model, n_head, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, hidden_dim, dropout=dropout)\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        x = x + self.attn(self.norm1(x), attention_mask)\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# --- 6. Advanced Classifier Model ---\n",
    "class AdvancedNewsClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels, d_model=512, n_head=16, n_layer=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_head, d_model * 4, dropout)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        self.norm_f = RMSNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # input_ids: (B, T)\n",
    "        x = self.embedding(input_ids)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask)\n",
    "            \n",
    "        x = self.norm_f(x)\n",
    "        \n",
    "        # Pooling: Use CLS token (index 0)\n",
    "        cls_repr = x[:, 0, :]\n",
    "        logits = self.classifier(self.dropout(cls_repr))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397289a",
   "metadata": {},
   "source": [
    "## 3. 训练流程 (Training Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3fd22e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def plot_training_history(history, title=\"Training History\"):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "    plt.title(f'{title} - Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')\n",
    "    plt.title(f'{title} - Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device, epoch=1, num_epochs=1, log_every=2000):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(dataloader, start=1):\n",
    "        # non_blocking=True 可以加速数据传输 (配合 pin_memory=True)\n",
    "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "        labels = batch['labels'].to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "        \n",
    "        if step % log_every == 0:\n",
    "             print(f\"Epoch {epoch}/{num_epochs} | Step {step}/{len(dataloader)} | loss={loss.item():.4f}\")\n",
    "        \n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "            labels = batch['labels'].to(device, non_blocking=True)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    return correct / total\n",
    "\n",
    "def run_training(text_key, save_name, epochs=10, batch_size=128): # 默认 batch_size 增加到 128\n",
    "    print(f\"\\n{'='*20} Training Advanced Model on {text_key} {'='*20}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Prepare Data\n",
    "    tokenizer = WhitespaceTokenizer(max_vocab_size=50000, max_length=128)\n",
    "    train_ds, val_ds, meta_ds, label2id, id2label = prepare_data(records, text_key, tokenizer)\n",
    "    \n",
    "    # 开启 pin_memory 加速 CPU 到 GPU/MPS 的传输\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, pin_memory=True)\n",
    "    \n",
    "    # 2. Initialize Model\n",
    "    model = AdvancedNewsClassifier(\n",
    "        vocab_size=len(tokenizer.token_to_id),\n",
    "        num_labels=len(label2id),\n",
    "        d_model=512,      # 扩大维度\n",
    "        n_head=16,        # 增加头数\n",
    "        n_layer=6,        # 增加层数\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    history = {'train_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, epoch=epoch+1, num_epochs=epochs)\n",
    "        val_acc = evaluate(model, val_loader, device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'label2id': label2id,\n",
    "                'id2label': id2label,\n",
    "                'vocab': tokenizer.token_to_id,\n",
    "                'config': {\n",
    "                    'd_model': 512,\n",
    "                    'n_head': 16,\n",
    "                    'n_layer': 6,\n",
    "                    'max_length': tokenizer.max_length,\n",
    "                    'text_key': text_key\n",
    "                }\n",
    "            }, save_name)\n",
    "            print(f\"Saved best model to {save_name}\")\n",
    "            \n",
    "    end_time = time.time()\n",
    "    print(f\"Training finished in {end_time - start_time:.2f}s\")\n",
    "    return best_acc, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea433f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 开始串行训练 Advanced 模型 ====================\n",
      "\n",
      "==================== Training Advanced Model on headline ====================\n",
      "Pre-tokenizing dataset (this may take a moment)...\n",
      "Pre-tokenization complete.\n",
      "Model Parameters: 42.78M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | Loss: 1.1129 | Val Acc: 0.6915\n",
      "Saved best model to ../Results/second/best_model_headline.pt\n"
     ]
    }
   ],
   "source": [
    "# --- 运行训练 (串行) ---\n",
    "import time\n",
    "\n",
    "os.makedirs(\"../Results/second\", exist_ok=True)\n",
    "\n",
    "print(f\"\\n{'='*20} 开始串行训练 Advanced 模型 {'='*20}\")\n",
    "start_time_all = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# 串行训练两个模型\n",
    "\n",
    "epochs = 12\n",
    "\n",
    "# 先训练 Headline\n",
    "acc_h, history_h = run_training(\n",
    "    text_key=\"headline\", \n",
    "    save_name=\"../Results/second/best_model_headline.pt\", \n",
    "    epochs=epochs, \n",
    "    batch_size=128\n",
    ")\n",
    "plot_training_history(history_h, title=\"Headline Model\")\n",
    "\n",
    "# 再训练 Description\n",
    "acc_d, history_d = run_training(\n",
    "    text_key=\"short_description\", \n",
    "    save_name=\"../Results/second/best_model_description.pt\", \n",
    "    epochs=epochs, \n",
    "    batch_size=128\n",
    ")\n",
    "plot_training_history(history_d, title=\"Description Model\")\n",
    "\n",
    "end_time_all = time.time()\n",
    "print(f\"\\n串行训练结束，总耗时: {end_time_all - start_time_all:.2f}s\")\n",
    "print(f\"Headline Best Acc: {acc_h:.4f}\")\n",
    "print(f\"Description Best Acc: {acc_d:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a8f04",
   "metadata": {},
   "source": [
    "## 4. Meta Model (Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777c6dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Meta Model...\n",
      "Meta Epoch 0: Loss 2.0972 Acc 0.1965\n",
      "Meta Epoch 5: Loss 2.0662 Acc 0.2113\n",
      "Meta Epoch 10: Loss 2.0368 Acc 0.2116\n",
      "Meta Epoch 15: Loss 2.0074 Acc 0.2119\n",
      "Meta Model Saved.\n",
      "Meta Model Training finished in 361.95s\n"
     ]
    }
   ],
   "source": [
    "class MetaClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), \n",
    "            nn.ReLU(),                        \n",
    "            nn.Dropout(0.2),                  \n",
    "            nn.Linear(hidden_dim, output_dim) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def load_advanced_model(path, device):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    vocab = checkpoint[\"vocab\"]\n",
    "    cfg = checkpoint[\"config\"]\n",
    "    \n",
    "    tokenizer = WhitespaceTokenizer(max_vocab_size=len(vocab), max_length=cfg['max_length'])\n",
    "    tokenizer.token_to_id = vocab\n",
    "    tokenizer.id_to_token = {v: k for k, v in vocab.items()}\n",
    "    \n",
    "    model = AdvancedNewsClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        num_labels=len(checkpoint['label2id']),\n",
    "        d_model=cfg['d_model'],\n",
    "        n_head=cfg['n_head'],\n",
    "        n_layer=cfg['n_layer']\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model, tokenizer, checkpoint['label2id']\n",
    "\n",
    "def train_meta_model():\n",
    "    print(\"\\nTraining Meta Model...\")\n",
    "    start_time = time.time()\n",
    "    # Load Models\n",
    "    model_h, tok_h, label2id = load_advanced_model(\"../Results/second/best_model_headline.pt\", device)\n",
    "    model_d, tok_d, _ = load_advanced_model(\"../Results/second/best_model_description.pt\", device)\n",
    "    \n",
    "    # Prepare Meta Data\n",
    "    # Re-split to get meta indices (must match original split)\n",
    "    full_size = len(records)\n",
    "    train_size = int(0.7 * full_size)\n",
    "    val_size = int(0.15 * full_size)\n",
    "    meta_size = full_size - train_size - val_size\n",
    "    _, _, meta_subset = random_split(\n",
    "        range(full_size), [train_size, val_size, meta_size], \n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    X, y = [], []\n",
    "    with torch.no_grad():\n",
    "        for idx in meta_subset.indices:\n",
    "            item = records[idx]\n",
    "            if item['category'] not in label2id: continue\n",
    "            target = label2id[item['category']]\n",
    "            \n",
    "            # Headline Pred\n",
    "            ids_h, mask_h = tok_h.encode(item.get('headline', ''))\n",
    "            logits_h = model_h(torch.tensor([ids_h]).to(device), torch.tensor([mask_h]).to(device))\n",
    "            probs_h = F.softmax(logits_h, dim=1).cpu().tolist()[0]\n",
    "            \n",
    "            # Desc Pred\n",
    "            ids_d, mask_d = tok_d.encode(item.get('short_description', ''))\n",
    "            logits_d = model_d(torch.tensor([ids_d]).to(device), torch.tensor([mask_d]).to(device))\n",
    "            probs_d = F.softmax(logits_d, dim=1).cpu().tolist()[0]\n",
    "            \n",
    "            X.append(probs_h + probs_d)\n",
    "            y.append(target)\n",
    "            \n",
    "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Train\n",
    "    meta_model = MetaClassifier(len(label2id)*2, 64, len(label2id)).to(device)\n",
    "    optimizer = torch.optim.Adam(meta_model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        optimizer.zero_grad()\n",
    "        out = meta_model(X)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            acc = (torch.argmax(out, dim=1) == y).float().mean()\n",
    "            print(f\"Meta Epoch {epoch}: Loss {loss.item():.4f} Acc {acc:.4f}\")\n",
    "            \n",
    "    torch.save(meta_model.state_dict(), \"../Results/second/best_meta_model.pt\")\n",
    "    print(\"Meta Model Saved.\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Meta Model Training finished in {end_time - start_time:.2f}s\")\n",
    "\n",
    "train_meta_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
